{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-08 09:01:08.049322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from textblob_de import TextBlobDE\n",
    "import nbformat\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud \n",
    "from functions import *\n",
    "from gensim.utils import simple_preprocess\n",
    "from globalvars import *\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown as md\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandas_profiling import ProfileReport\n",
    "import contractions\n",
    "import gensim\n",
    "import kaleido\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from functions import *\n",
    "import csv\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# nlp = spacy.load(\"de_core_news_lg\")\n",
    "# ## download nlp language package\n",
    "# #!python -m spacy download de_core_news_lg\n",
    "# # Load the German model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe\n",
    "filelocation = 'data/DataText'\n",
    "df = pd.read_feather(filelocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>u_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>yearmonth</th>\n",
       "      <th>yearquarter</th>\n",
       "      <th>season</th>\n",
       "      <th>Kommentar</th>\n",
       "      <th>wime_personal</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_zielort</th>\n",
       "      <th>Kommentar_Character</th>\n",
       "      <th>Kommentar_Tokens</th>\n",
       "      <th>Kommentar_Types</th>\n",
       "      <th>Kommentar_TTR</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns_and_adjectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42319</td>\n",
       "      <td>2019-05-28</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>2019/2</td>\n",
       "      <td>spring</td>\n",
       "      <td>Bitte nicht alles elektronifizieren !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Thun</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>bitte elektronifizieren !</td>\n",
       "      <td>[bitte, elektronifizieren, !]</td>\n",
       "      <td>[bitte, elektronifizier, --]</td>\n",
       "      <td>[elektronifizier]</td>\n",
       "      <td>[elektronifizier]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42429</td>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Bei der Ankunft in Olten um Mitternacht 24 00 ...</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>Zofingen</td>\n",
       "      <td>284</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>ankunft mitternacht 24 00 48 min. gewartet näc...</td>\n",
       "      <td>[ankunft, mitternacht, 24, 00, 48, min., gewar...</td>\n",
       "      <td>[ankunft, mitternacht, 24, 00, 48, min., warte...</td>\n",
       "      <td>[ankunft, mitternacht, anschluss]</td>\n",
       "      <td>[ankunft, mitternacht, nächster, anschluss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41926</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>im Zug 6.25 ab St. Gallen sind sehr viel die W...</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>Winterthur</td>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.25 wc's gesperrt.</td>\n",
       "      <td>[6.25, wc's, gesperrt, .]</td>\n",
       "      <td>[6.25, wc's, sperren, --]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42403</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>BAHNHOF PRATTELN WIEDER BEDIENEN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Interlaken West</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>bahnhof bedienen</td>\n",
       "      <td>[bahnhof, bedienen]</td>\n",
       "      <td>[bahnhof, bedienen]</td>\n",
       "      <td>[bahnhof]</td>\n",
       "      <td>[bahnhof]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42486</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Ich kann nicht verstehen, warum mit dem Fahrpl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Zürich HB</td>\n",
       "      <td>127</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>verstehen, fahrplanwechsel langer doppelstockz...</td>\n",
       "      <td>[verstehen, ,, fahrplanwechsel, langer, doppel...</td>\n",
       "      <td>[verstehen, --, fahrplanwechsel, lang, doppels...</td>\n",
       "      <td>[doppelstockzug]</td>\n",
       "      <td>[doppelstockzug]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>51510</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Ich finde auf dem SBB App nirgens eine Option ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Baar</td>\n",
       "      <td>157</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>finde sbb app nirgens option nachtzuschlag. me...</td>\n",
       "      <td>[finde, sbb, app, nirgens, option, nachtzuschl...</td>\n",
       "      <td>[finden, sbb, app, nirgen, option, nachtzuschl...</td>\n",
       "      <td>[app, nirgen, option, meinung]</td>\n",
       "      <td>[app, nirgen, option, meinung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>51505</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>WC's sollten nicht systemmässig geschlossen se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Brig Bahnhofplatz</td>\n",
       "      <td>124</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>92.857143</td>\n",
       "      <td>wc's systemmässig geschlossen sein. stosszeite...</td>\n",
       "      <td>[wc's, systemmässig, geschlossen, sein, ., sto...</td>\n",
       "      <td>[wc's, systemmässig, schließen, sein, --, stos...</td>\n",
       "      <td>[platzangebot, zugskompositionen]</td>\n",
       "      <td>[grössere, platzangebot, zugskompositionen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>51491</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>mehr platz zur verfügung stellen... jedesmal m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>96</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>platz verfügung stellen... jedesmal stehen. mü...</td>\n",
       "      <td>[platz, verfügung, stellen, ..., jedesmal, ste...</td>\n",
       "      <td>[platz, verfügung, stellen, --, jedesmal, steh...</td>\n",
       "      <td>[platz, verfügung, vorallem, preis]</td>\n",
       "      <td>[platz, verfügung, vorallem, preis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>51490</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Die Anschluss-Verbindungen von oder nach Galge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Landquart</td>\n",
       "      <td>197</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>96.551724</td>\n",
       "      <td>anschluss-verbindungen bahnhof siebnen (zu sch...</td>\n",
       "      <td>[anschluss-verbindungen, bahnhof, siebnen, (, ...</td>\n",
       "      <td>[anschluss-verbindungen, bahnhof, siebn, --, z...</td>\n",
       "      <td>[anschluss-verbindungen, bahnhof, richtung, fa...</td>\n",
       "      <td>[anschluss-verbindungen, bahnhof, richtung, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>51527</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Mehr Spielwagen auf unterschiedlichen Strecken...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Chur</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>spielwagen unterschiedlichen strecken wären su...</td>\n",
       "      <td>[spielwagen, unterschiedlichen, strecken, wäre...</td>\n",
       "      <td>[spielwag, unterschiedlich, strecke, sein, sup...</td>\n",
       "      <td>[strecke]</td>\n",
       "      <td>[strecke]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant_id     u_date  year  month  quarter  yearmonth yearquarter  \\\n",
       "0             42319 2019-05-28  2019      5        2 2019-05-01      2019/2   \n",
       "1             42429 2019-02-13  2019      2        1 2019-02-01      2019/1   \n",
       "2             41926 2019-02-08  2019      2        1 2019-02-01      2019/1   \n",
       "3             42403 2019-02-07  2019      2        1 2019-02-01      2019/1   \n",
       "4             42486 2019-02-06  2019      2        1 2019-02-01      2019/1   \n",
       "..              ...        ...   ...    ...      ...        ...         ...   \n",
       "385           51510 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "386           51505 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "387           51491 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "388           51490 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "389           51527 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "\n",
       "     season                                          Kommentar  wime_personal  \\\n",
       "0    spring              Bitte nicht alles elektronifizieren !            NaN   \n",
       "1    winter  Bei der Ankunft in Olten um Mitternacht 24 00 ...      77.777778   \n",
       "2    winter  im Zug 6.25 ab St. Gallen sind sehr viel die W...      77.777778   \n",
       "3    winter                   BAHNHOF PRATTELN WIEDER BEDIENEN     100.000000   \n",
       "4    winter  Ich kann nicht verstehen, warum mit dem Fahrpl...            NaN   \n",
       "..      ...                                                ...            ...   \n",
       "385  winter  Ich finde auf dem SBB App nirgens eine Option ...            NaN   \n",
       "386  winter  WC's sollten nicht systemmässig geschlossen se...            NaN   \n",
       "387  winter  mehr platz zur verfügung stellen... jedesmal m...            NaN   \n",
       "388  winter  Die Anschluss-Verbindungen von oder nach Galge...            NaN   \n",
       "389  winter  Mehr Spielwagen auf unterschiedlichen Strecken...     100.000000   \n",
       "\n",
       "     ...         ft_zielort  Kommentar_Character  Kommentar_Tokens  \\\n",
       "0    ...               Thun                   37                 5   \n",
       "1    ...           Zofingen                  284                50   \n",
       "2    ...         Winterthur                   65                13   \n",
       "3    ...    Interlaken West                   32                 4   \n",
       "4    ...          Zürich HB                  127                18   \n",
       "..   ...                ...                  ...               ...   \n",
       "385  ...               Baar                  157                24   \n",
       "386  ...  Brig Bahnhofplatz                  124                14   \n",
       "387  ...             Luzern                   96                14   \n",
       "388  ...          Landquart                  197                29   \n",
       "389  ...               Chur                   59                 7   \n",
       "\n",
       "     Kommentar_Types  Kommentar_TTR  \\\n",
       "0                  5     100.000000   \n",
       "1                 46      92.000000   \n",
       "2                 13     100.000000   \n",
       "3                  4     100.000000   \n",
       "4                 18     100.000000   \n",
       "..               ...            ...   \n",
       "385               24     100.000000   \n",
       "386               13      92.857143   \n",
       "387               14     100.000000   \n",
       "388               28      96.551724   \n",
       "389                7     100.000000   \n",
       "\n",
       "                                     text_preprocessed  \\\n",
       "0                            bitte elektronifizieren !   \n",
       "1    ankunft mitternacht 24 00 48 min. gewartet näc...   \n",
       "2                                  6.25 wc's gesperrt.   \n",
       "3                                     bahnhof bedienen   \n",
       "4    verstehen, fahrplanwechsel langer doppelstockz...   \n",
       "..                                                 ...   \n",
       "385  finde sbb app nirgens option nachtzuschlag. me...   \n",
       "386  wc's systemmässig geschlossen sein. stosszeite...   \n",
       "387  platz verfügung stellen... jedesmal stehen. mü...   \n",
       "388  anschluss-verbindungen bahnhof siebnen (zu sch...   \n",
       "389  spielwagen unterschiedlichen strecken wären su...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "0                        [bitte, elektronifizieren, !]   \n",
       "1    [ankunft, mitternacht, 24, 00, 48, min., gewar...   \n",
       "2                            [6.25, wc's, gesperrt, .]   \n",
       "3                                  [bahnhof, bedienen]   \n",
       "4    [verstehen, ,, fahrplanwechsel, langer, doppel...   \n",
       "..                                                 ...   \n",
       "385  [finde, sbb, app, nirgens, option, nachtzuschl...   \n",
       "386  [wc's, systemmässig, geschlossen, sein, ., sto...   \n",
       "387  [platz, verfügung, stellen, ..., jedesmal, ste...   \n",
       "388  [anschluss-verbindungen, bahnhof, siebnen, (, ...   \n",
       "389  [spielwagen, unterschiedlichen, strecken, wäre...   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "0                         [bitte, elektronifizier, --]   \n",
       "1    [ankunft, mitternacht, 24, 00, 48, min., warte...   \n",
       "2                            [6.25, wc's, sperren, --]   \n",
       "3                                  [bahnhof, bedienen]   \n",
       "4    [verstehen, --, fahrplanwechsel, lang, doppels...   \n",
       "..                                                 ...   \n",
       "385  [finden, sbb, app, nirgen, option, nachtzuschl...   \n",
       "386  [wc's, systemmässig, schließen, sein, --, stos...   \n",
       "387  [platz, verfügung, stellen, --, jedesmal, steh...   \n",
       "388  [anschluss-verbindungen, bahnhof, siebn, --, z...   \n",
       "389  [spielwag, unterschiedlich, strecke, sein, sup...   \n",
       "\n",
       "                                                 nouns  \\\n",
       "0                                    [elektronifizier]   \n",
       "1                    [ankunft, mitternacht, anschluss]   \n",
       "2                                                   []   \n",
       "3                                            [bahnhof]   \n",
       "4                                     [doppelstockzug]   \n",
       "..                                                 ...   \n",
       "385                     [app, nirgen, option, meinung]   \n",
       "386                  [platzangebot, zugskompositionen]   \n",
       "387                [platz, verfügung, vorallem, preis]   \n",
       "388  [anschluss-verbindungen, bahnhof, richtung, fa...   \n",
       "389                                          [strecke]   \n",
       "\n",
       "                                  nouns_and_adjectives  \n",
       "0                                    [elektronifizier]  \n",
       "1          [ankunft, mitternacht, nächster, anschluss]  \n",
       "2                                                   []  \n",
       "3                                            [bahnhof]  \n",
       "4                                     [doppelstockzug]  \n",
       "..                                                 ...  \n",
       "385                     [app, nirgen, option, meinung]  \n",
       "386        [grössere, platzangebot, zugskompositionen]  \n",
       "387                [platz, verfügung, vorallem, preis]  \n",
       "388  [anschluss-verbindungen, bahnhof, richtung, fa...  \n",
       "389                                          [strecke]  \n",
       "\n",
       "[390 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/12/how-to-extract-key-phrases-using-tfidf-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['participant_id', 'u_date', 'year', 'month', 'quarter', 'yearmonth',\n",
       "       'yearquarter', 'season', 'Kommentar', 'wime_personal', 'wime_komfort',\n",
       "       'wime_sauberkeit', 'wime_puenktlich', 'wime_platzangebot',\n",
       "       'wime_gesamtzuf', 'wime_preis_leistung', 'wime_fahrplan',\n",
       "       'wime_oes_fahrt', 'S_sprache', 'S_alter', 'S_sex', 'S_wohnsitz',\n",
       "       'u_klassencode', 'S_AB3_HTA', 'R_anschluss', 'R_stoerung',\n",
       "       'device_type', 'dispcode', 'u_ticket', 'u_fahrausweis', 'u_preis',\n",
       "       'R_zweck', 'ft_abfahrt', 'ft_ankunft', 'ft_startort_uic', 'ft_tu',\n",
       "       'ft_vm', 'ft_vm_kurz', 'ft_zielort_uic', 'fg_abfahrt', 'fg_ankunft',\n",
       "       'fg_startort_uic', 'fg_zielort_uic', 'fg_startort', 'fg_zielort',\n",
       "       'ft_startort', 'ft_zielort', 'Kommentar_Character', 'Kommentar_Tokens',\n",
       "       'Kommentar_Types', 'Kommentar_TTR', 'text_preprocessed', 'tokenized',\n",
       "       'lemmatized', 'lemmatized_no_loc', 'nouns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.u_ticket\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = join_list_of_list(df['nouns'])\n",
    "\n",
    "# Comparing two corpus\n",
    "corpus_1 = df[df['S_sex'] == 'weiblich']\n",
    "corpus_1 =join_list_of_list(corpus_1['nouns'])\n",
    "\n",
    "corpus_2 = df[df['S_sex'] == 'männlich']\n",
    "corpus_2 =join_list_of_list(corpus_2['nouns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vectorizer_params = {'ngram_range': (2,3), 'max_df': 0.99,'min_df':1}\n",
    "get_tfidf(corpus=corpus,vectorizer_params=vectorizer_params,n_keywords=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "züge            224.122495\n",
       "leute           179.358784\n",
       "platz           167.735646\n",
       "menschen        167.150151\n",
       "kinderwagen     157.271480\n",
       "strecke         148.612638\n",
       "bahnhof         143.674114\n",
       "sitzplätze      128.435955\n",
       "zügen           125.019717\n",
       "toiletten       121.059645\n",
       "maske           120.052572\n",
       "sauberkeit      116.224008\n",
       "verbindungen    114.271775\n",
       "gepäck          112.566586\n",
       "fahrt           103.965967\n",
       "verspätung       99.424391\n",
       "verbindung       99.239835\n",
       "klasse           93.173354\n",
       "koffer           92.472989\n",
       "masken           83.961235\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_params = {'ngram_range': (1,2), 'max_df': 0.1,'min_df':10}\n",
    "compare_tfidf(corpus_2,corpus_1,vectorizer_params=vectorizer_params,n_keywords=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords(texts, num_keywords=10):\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    dense = tfidf_matrix.todense()\n",
    "    denselist = dense.tolist()\n",
    "    texts_keywords = [\n",
    "        [(feature_names[index], scores) for index, scores in zip(text.indices, text.data) if scores > 0]\n",
    "        for text in denselist\n",
    "    ]\n",
    "\n",
    "    keywords = {}\n",
    "    for text_keywords in texts_keywords:\n",
    "        for word, score in text_keywords:\n",
    "            if word in keywords:\n",
    "                keywords[word] += score\n",
    "            else:\n",
    "                keywords[word] = score\n",
    "\n",
    "    keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    keywords = keywords[:num_keywords]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "extract_keywords(df['Kommentar'], num_keywords=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['nouns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trending_df \u001b[39m=\u001b[39m calculate_trending_keywords_tfidf(df\u001b[39m=\u001b[39;49mdf,group_column\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mseason\u001b[39;49m\u001b[39m'\u001b[39;49m,text_column\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnouns\u001b[39;49m\u001b[39m'\u001b[39;49m,min_df\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,max_df\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mcalculate_trending_keywords_tfidf\u001b[0;34m(df, group_column, text_column, min_df, max_df)\u001b[0m\n\u001b[1;32m     12\u001b[0m group_text \u001b[39m=\u001b[39m group_text\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcat(sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Extract the keywords and their scores\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform([group_text])\n\u001b[1;32m     15\u001b[0m feature_names \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[1;32m     16\u001b[0m scores \u001b[39m=\u001b[39m tfidf_matrix\u001b[39m.\u001b[39mtoarray()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2116\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2117\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2118\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2119\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2120\u001b[0m )\n\u001b[0;32m-> 2121\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2123\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1390\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1389\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[0;32m-> 1390\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_limit_features(\n\u001b[1;32m   1391\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[1;32m   1392\u001b[0m )\n\u001b[1;32m   1393\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1242\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1240\u001b[0m kept_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(mask)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kept_indices) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1244\u001b[0m     )\n\u001b[1;32m   1245\u001b[0m \u001b[39mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "trending_df = calculate_trending_keywords_tfidf(df=df,group_column='season',text_column='nouns',min_df=0.1,max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Score</th>\n",
       "      <th>Group</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count Relative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zug</td>\n",
       "      <td>0.557148</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.273441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klasse</td>\n",
       "      <td>0.241484</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.118311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbb</td>\n",
       "      <td>0.238327</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.116838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zürich</td>\n",
       "      <td>0.190977</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.093765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bern</td>\n",
       "      <td>0.164146</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.080511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>züge</td>\n",
       "      <td>0.138766</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.069292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>verbindung</td>\n",
       "      <td>0.126659</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>7</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.063310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bahnhof</td>\n",
       "      <td>0.120140</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.059821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ist</td>\n",
       "      <td>0.120140</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>9</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.059821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bern</td>\n",
       "      <td>0.118277</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Keyword     Score      Group Rank  Count  Count Relative\n",
       "0          zug  0.557148 2022-12-01    1  0.557        0.273441\n",
       "1       klasse  0.241484 2022-12-01    2  0.241        0.118311\n",
       "2          sbb  0.238327 2022-12-01    3  0.238        0.116838\n",
       "3       zürich  0.190977 2022-12-01    4  0.191        0.093765\n",
       "4         bern  0.164146 2022-12-01    5  0.164        0.080511\n",
       "..         ...       ...        ...  ...    ...             ...\n",
       "5         züge  0.138766 2019-01-01    6  0.139        0.069292\n",
       "6   verbindung  0.126659 2019-01-01    7  0.127        0.063310\n",
       "7      bahnhof  0.120140 2019-01-01    8  0.120        0.059821\n",
       "8          ist  0.120140 2019-01-01    9  0.120        0.059821\n",
       "9         bern  0.118277 2019-01-01   10  0.118        0.058824\n",
       "\n",
       "[460 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trending_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword 1</th>\n",
       "      <th>Keyword 2</th>\n",
       "      <th>Keyword 3</th>\n",
       "      <th>Keyword 4</th>\n",
       "      <th>Keyword 5</th>\n",
       "      <th>Keyword 6</th>\n",
       "      <th>Keyword 7</th>\n",
       "      <th>Keyword 8</th>\n",
       "      <th>Keyword 9</th>\n",
       "      <th>Keyword 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>klasse</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>bern</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>ist</td>\n",
       "      <td>platz</td>\n",
       "      <td>app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>app</td>\n",
       "      <td>bern</td>\n",
       "      <td>wagen</td>\n",
       "      <td>fahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>züge</td>\n",
       "      <td>platz</td>\n",
       "      <td>wagen</td>\n",
       "      <td>bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>zürich</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>platz</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>züge</td>\n",
       "      <td>bern</td>\n",
       "      <td>werden</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bern</td>\n",
       "      <td>züge</td>\n",
       "      <td>wagen</td>\n",
       "      <td>platz</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>platz</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>wagen</td>\n",
       "      <td>app</td>\n",
       "      <td>bern</td>\n",
       "      <td>bahnhof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>züge</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>app</td>\n",
       "      <td>platz</td>\n",
       "      <td>bern</td>\n",
       "      <td>wagen</td>\n",
       "      <td>bahnhof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>platz</td>\n",
       "      <td>ist</td>\n",
       "      <td>wagen</td>\n",
       "      <td>app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>zürich</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>züge</td>\n",
       "      <td>bern</td>\n",
       "      <td>strecke</td>\n",
       "      <td>ist</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>strecke</td>\n",
       "      <td>platz</td>\n",
       "      <td>werden</td>\n",
       "      <td>züge</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bus</td>\n",
       "      <td>züge</td>\n",
       "      <td>ist</td>\n",
       "      <td>platz</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>ist</td>\n",
       "      <td>maske</td>\n",
       "      <td>zügen</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>maske</td>\n",
       "      <td>züge</td>\n",
       "      <td>ist</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>bern</td>\n",
       "      <td>fahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>bus</td>\n",
       "      <td>maske</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>strecke</td>\n",
       "      <td>ist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>app</td>\n",
       "      <td>züge</td>\n",
       "      <td>werden</td>\n",
       "      <td>bern</td>\n",
       "      <td>ist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>klasse</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>strecke</td>\n",
       "      <td>werden</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>ist</td>\n",
       "      <td>app</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>werden</td>\n",
       "      <td>zügen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>bus</td>\n",
       "      <td>klasse</td>\n",
       "      <td>werden</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>ist</td>\n",
       "      <td>platz</td>\n",
       "      <td>bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>züge</td>\n",
       "      <td>zügen</td>\n",
       "      <td>werden</td>\n",
       "      <td>ist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>züge</td>\n",
       "      <td>klasse</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>werden</td>\n",
       "      <td>bern</td>\n",
       "      <td>platz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>klasse</td>\n",
       "      <td>app</td>\n",
       "      <td>finde</td>\n",
       "      <td>ist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>ist</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>corona</td>\n",
       "      <td>bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>corona</td>\n",
       "      <td>app</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>werden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>zürich</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>corona</td>\n",
       "      <td>ist</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>werden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>app</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>ist</td>\n",
       "      <td>finde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>finde</td>\n",
       "      <td>klasse</td>\n",
       "      <td>maske</td>\n",
       "      <td>bus</td>\n",
       "      <td>fahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>ist</td>\n",
       "      <td>verbindung</td>\n",
       "      <td>maskenpflicht</td>\n",
       "      <td>fahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>maskenpflicht</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>maske</td>\n",
       "      <td>bus</td>\n",
       "      <td>verbindung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>maskenpflicht</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>maske</td>\n",
       "      <td>ist</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>maskenpflicht</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>züge</td>\n",
       "      <td>bus</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>finde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>züge</td>\n",
       "      <td>ist</td>\n",
       "      <td>bus</td>\n",
       "      <td>ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>züge</td>\n",
       "      <td>klasse</td>\n",
       "      <td>verspätung</td>\n",
       "      <td>strecke</td>\n",
       "      <td>bern</td>\n",
       "      <td>bahnhof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>bern</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>bus</td>\n",
       "      <td>bahnhof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>züge</td>\n",
       "      <td>bern</td>\n",
       "      <td>werden</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>züge</td>\n",
       "      <td>verspätung</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>klasse</td>\n",
       "      <td>bern</td>\n",
       "      <td>minuten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>platz</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>wagen</td>\n",
       "      <td>strecke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>app</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>platz</td>\n",
       "      <td>bern</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>klasse</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>platz</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>strecke</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>klasse</td>\n",
       "      <td>strecke</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>platz</td>\n",
       "      <td>fahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>app</td>\n",
       "      <td>platz</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>wagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>züge</td>\n",
       "      <td>bern</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>strecke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>platz</td>\n",
       "      <td>züge</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>ist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>klasse</td>\n",
       "      <td>app</td>\n",
       "      <td>platz</td>\n",
       "      <td>züge</td>\n",
       "      <td>ist</td>\n",
       "      <td>bern</td>\n",
       "      <td>bahnhof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>app</td>\n",
       "      <td>zürich</td>\n",
       "      <td>züge</td>\n",
       "      <td>klasse</td>\n",
       "      <td>fahrt</td>\n",
       "      <td>bern</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>verbindung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>zug</td>\n",
       "      <td>sbb</td>\n",
       "      <td>zürich</td>\n",
       "      <td>app</td>\n",
       "      <td>klasse</td>\n",
       "      <td>züge</td>\n",
       "      <td>verbindung</td>\n",
       "      <td>bahnhof</td>\n",
       "      <td>ist</td>\n",
       "      <td>bern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Keyword 1 Keyword 2      Keyword 3      Keyword 4      Keyword 5  \\\n",
       "2022-12-01       zug    klasse            sbb         zürich           bern   \n",
       "2022-11-01       zug       sbb         zürich         klasse           züge   \n",
       "2022-10-01       zug       sbb         zürich            app         klasse   \n",
       "2022-09-01       zug    zürich            sbb         klasse          platz   \n",
       "2022-08-01       zug       sbb         zürich         klasse           bern   \n",
       "2022-07-01       zug       sbb         zürich          platz         klasse   \n",
       "2022-06-01       zug       sbb           züge         zürich         klasse   \n",
       "2022-05-01       zug       sbb         klasse         zürich           züge   \n",
       "2022-04-01       zug    zürich            sbb         klasse        bahnhof   \n",
       "2022-03-01       zug       sbb         klasse         zürich        bahnhof   \n",
       "2022-02-01       zug       sbb         zürich         klasse            bus   \n",
       "2022-01-01       zug       sbb         klasse         zürich           züge   \n",
       "2021-12-01       zug       sbb         klasse         zürich          maske   \n",
       "2021-11-01       zug       sbb         klasse         zürich           züge   \n",
       "2021-10-01       zug       sbb         zürich         klasse        bahnhof   \n",
       "2021-09-01       zug    klasse            sbb         zürich           züge   \n",
       "2021-08-01       zug       sbb         zürich         klasse            ist   \n",
       "2021-07-01       zug       sbb            app        bahnhof         zürich   \n",
       "2021-06-01       zug       sbb            app        bahnhof         zürich   \n",
       "2021-05-01       zug       sbb            app         klasse         zürich   \n",
       "2021-04-01       zug       sbb        bahnhof           züge         klasse   \n",
       "2021-03-01       zug       sbb         zürich           züge        bahnhof   \n",
       "2021-02-01       zug       sbb            ist         klasse         zürich   \n",
       "2021-01-01       zug       sbb           züge        bahnhof         zürich   \n",
       "2020-12-01       zug       sbb         klasse           züge         zürich   \n",
       "2020-11-01       zug       sbb        bahnhof            app         klasse   \n",
       "2020-10-01       zug       sbb            app         zürich        bahnhof   \n",
       "2020-09-01       zug       sbb            app         zürich         klasse   \n",
       "2020-08-01       zug       sbb            app         zürich  maskenpflicht   \n",
       "2020-07-01       zug       sbb            app  maskenpflicht         zürich   \n",
       "2020-06-01       zug       sbb  maskenpflicht         zürich            app   \n",
       "2020-03-01       zug       sbb         klasse         zürich            app   \n",
       "2020-02-01       zug       sbb         zürich            app           züge   \n",
       "2020-01-01       zug       sbb            app         zürich         klasse   \n",
       "2019-12-01       zug       sbb         klasse         zürich            app   \n",
       "2019-11-01       zug       sbb         zürich            app           züge   \n",
       "2019-10-01       zug       sbb         zürich            app         klasse   \n",
       "2019-09-01       zug       sbb         klasse         zürich           züge   \n",
       "2019-08-01       zug       sbb         klasse         zürich            app   \n",
       "2019-07-01       zug       sbb            app         zürich           züge   \n",
       "2019-06-01       zug       sbb         zürich         klasse           züge   \n",
       "2019-05-01       zug       sbb            app         zürich         klasse   \n",
       "2019-04-01       zug       sbb            app         zürich         klasse   \n",
       "2019-03-01       zug       sbb         zürich         klasse            app   \n",
       "2019-02-01       zug       sbb            app         zürich           züge   \n",
       "2019-01-01       zug       sbb         zürich            app         klasse   \n",
       "\n",
       "             Keyword 6   Keyword 7   Keyword 8      Keyword 9  Keyword 10  \n",
       "2022-12-01        züge     bahnhof         ist          platz         app  \n",
       "2022-11-01     bahnhof         app        bern          wagen       fahrt  \n",
       "2022-10-01     bahnhof        züge       platz          wagen        bern  \n",
       "2022-09-01     bahnhof        züge        bern         werden       wagen  \n",
       "2022-08-01        züge       wagen       platz            app     bahnhof  \n",
       "2022-07-01        züge       wagen         app           bern     bahnhof  \n",
       "2022-06-01         app       platz        bern          wagen     bahnhof  \n",
       "2022-05-01     bahnhof       platz         ist          wagen         app  \n",
       "2022-04-01        züge        bern     strecke            ist       wagen  \n",
       "2022-03-01     strecke       platz      werden           züge       wagen  \n",
       "2022-02-01        züge         ist       platz        bahnhof        bern  \n",
       "2022-01-01         ist       maske       zügen        bahnhof         bus  \n",
       "2021-12-01        züge         ist     bahnhof           bern       fahrt  \n",
       "2021-11-01         bus       maske     bahnhof        strecke         ist  \n",
       "2021-10-01         app        züge      werden           bern         ist  \n",
       "2021-09-01         app     bahnhof     strecke         werden       wagen  \n",
       "2021-08-01         app        züge     bahnhof         werden       zügen  \n",
       "2021-07-01        züge         bus      klasse         werden       wagen  \n",
       "2021-06-01      klasse        züge         ist          platz        bern  \n",
       "2021-05-01     bahnhof        züge       zügen         werden         ist  \n",
       "2021-04-01         app      zürich      werden           bern       platz  \n",
       "2021-03-01       fahrt      klasse         app          finde         ist  \n",
       "2021-02-01        züge         app     bahnhof         corona        bern  \n",
       "2021-01-01      klasse      corona         app          fahrt      werden  \n",
       "2020-12-01       fahrt      corona         ist        bahnhof      werden  \n",
       "2020-11-01      zürich        züge       fahrt            ist       finde  \n",
       "2020-10-01       finde      klasse       maske            bus       fahrt  \n",
       "2020-09-01     bahnhof         ist  verbindung  maskenpflicht       fahrt  \n",
       "2020-08-01     bahnhof       fahrt       maske            bus  verbindung  \n",
       "2020-07-01      klasse       maske         ist        bahnhof      ticket  \n",
       "2020-06-01     bahnhof        züge         bus          fahrt       finde  \n",
       "2020-03-01       fahrt        züge         ist            bus      ticket  \n",
       "2020-02-01      klasse  verspätung     strecke           bern     bahnhof  \n",
       "2020-01-01        züge        bern       fahrt            bus     bahnhof  \n",
       "2019-12-01       fahrt        züge        bern         werden       wagen  \n",
       "2019-11-01  verspätung     bahnhof      klasse           bern     minuten  \n",
       "2019-10-01        züge       platz       fahrt          wagen     strecke  \n",
       "2019-09-01         app     bahnhof       platz           bern       wagen  \n",
       "2019-08-01       platz        züge     bahnhof        strecke       wagen  \n",
       "2019-07-01      klasse     strecke     bahnhof          platz       fahrt  \n",
       "2019-06-01         app       platz       fahrt        bahnhof       wagen  \n",
       "2019-05-01       fahrt        züge        bern        bahnhof     strecke  \n",
       "2019-04-01       platz        züge     bahnhof          fahrt         ist  \n",
       "2019-03-01       platz        züge         ist           bern     bahnhof  \n",
       "2019-02-01      klasse       fahrt        bern        bahnhof  verbindung  \n",
       "2019-01-01        züge  verbindung     bahnhof            ist        bern  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trending_df\n",
    "convert_to_wide_format(trending_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_tfidf_vectors(texts, ngram_range=(1,2)):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries containing the tf-idf scores for each term in each text.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, max_df=0.20, min_df=1, ngram_range=ngram_range)\n",
    "    vectors = vectorizer.fit_transform(texts)\n",
    "    dict_of_tokens = {i[1]:i[0] for i in vectorizer.vocabulary_.items()}\n",
    "    tfidf_vectors = []\n",
    "    for row in vectors:\n",
    "        tfidf_vectors.append({dict_of_tokens[column]:value for (column,value) in zip(row.indices,row.data)})\n",
    "    return tfidf_vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_top_keywords(corpus, N):\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, max_df=0.20,min_df=1, ngram_range=(1,2))\n",
    "    vectors = vectorizer.fit_transform(corpus)\n",
    "    scores = vectors.toarray().sum(axis=0)\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    keywords = [(term, scores[index]) for term, index in vocab.items()]\n",
    "    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "    top_keywords = keywords[:N]\n",
    "    return top_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['season'] == 'summer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nouns_location_removed_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nouns_location_removed_str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m texts \u001b[39m=\u001b[39m df_filtered[\u001b[39m'\u001b[39;49m\u001b[39mnouns_location_removed_str\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      2\u001b[0m tfidf_vectors \u001b[39m=\u001b[39m get_tfidf_vectors(texts, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nouns_location_removed_str'"
     ]
    }
   ],
   "source": [
    "texts = df_filtered['nouns_location_removed_str']\n",
    "tfidf_vectors = get_tfidf_vectors(texts, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = extract_top_keywords(texts, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sbb', 270.2272351753645),\n",
       " ('verbindung', 218.18687857058507),\n",
       " ('preis', 212.1203815765708),\n",
       " ('platz', 205.7054484544187),\n",
       " ('app', 199.2926111995926),\n",
       " ('verspätung', 163.32726414951395),\n",
       " ('züge', 158.22297928774728),\n",
       " ('strecke', 153.4774236069616),\n",
       " ('bahnhof', 144.32522531789604),\n",
       " ('maskenpflicht', 143.83418725673877),\n",
       " ('klimaanlage', 137.412951098461),\n",
       " ('bern', 130.91165784860297),\n",
       " ('züg', 129.20285116314722),\n",
       " ('zeit', 114.01342914811875),\n",
       " ('ticket', 110.85303105908879),\n",
       " ('bus', 109.689467968056),\n",
       " ('reise', 106.43615089106923),\n",
       " ('leute', 98.70640982763535),\n",
       " ('maske', 96.70920402192174),\n",
       " ('velo', 96.6874780255895)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RAKE-tutorial'...\n",
      "remote: Enumerating objects: 303, done.\u001b[K\n",
      "remote: Total 303 (delta 0), reused 0 (delta 0), pack-reused 303\u001b[K\n",
      "Receiving objects: 100% (303/303), 4.31 MiB | 1.09 MiB/s, done.\n",
      "Resolving deltas: 100% (50/50), done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in /Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from rake-nltk) (3.8.1)\n",
      "Requirement already satisfied: click in /Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install rake-nltk\n",
    "from rake_nltk import Rake\n",
    "import operator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_group_keywords(df, group_col, text_col, n=10, group_values=None, tfidf_params=None):\n",
    "    \"\"\"\n",
    "    Extracts the top n keywords for a given group or groups in a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing the data\n",
    "    - group_col (str): The name of the column in the dataframe that holds the group information\n",
    "    - text_col (str): The name of the column in the dataframe that holds the text information\n",
    "    - n (int, optional): The number of top keywords to extract. Default is 10\n",
    "    - group_values (tuple, optional): The values of the group or groups to extract the top keywords for.\n",
    "                                      Default is None, which means all groups.\n",
    "    - tfidf_params (dict, optional): Additional parameters to pass to the TfidfVectorizer class.\n",
    "                                     Default is None, which means no additional parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of the top n keywords\n",
    "    \n",
    "    \"\"\"\n",
    "    if group_values:\n",
    "        df = df[df[group_col].isin(group_values)]\n",
    "\n",
    "    corpus = df[text_col].apply(lambda x: \" \".join(x)).tolist()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(**(tfidf_params if tfidf_params else {}))\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "    top_keywords = []\n",
    "    for i, row in tfidf_scores.iterrows():\n",
    "        sorted_words = row.sort_values(ascending=False).index.tolist()\n",
    "        top_keywords += sorted_words[:n]\n",
    "        \n",
    "    return top_keywords[:n]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_group_keywords(df, group_col, text_col, n=10, group_values=None, tfidf_params=None):\n",
    "    \"\"\"\n",
    "    Extract the top `n` keywords that are more present in a subgroup compared to the rest of the corpus (all other groups).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    group_col : str\n",
    "        Column in `df` that contains the subgroup information.\n",
    "    text_col : str\n",
    "        Column in `df` that contains the text data.\n",
    "    n : int, optional (default=10)\n",
    "        Number of keywords to extract for each subgroup.\n",
    "    group_values : list, optional (default=None)\n",
    "        List of values in `group_col` to include in the analysis. If None, include all unique values.\n",
    "    tfidf_params : dict, optional (default=None)\n",
    "        Dictionary of parameters to pass to `TfidfVectorizer`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    group_keywords : dict\n",
    "        Dictionary where the keys are the subgroup names and the values are lists of the top `n` keywords\n",
    "        that are more present in that subgroup compared to the rest of the corpus (all other groups).\n",
    "    \"\"\"\n",
    "    if group_values:\n",
    "        df = df[df[group_col].isin(group_values)]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**(tfidf_params if tfidf_params else {}))\n",
    "    X = vectorizer.fit_transform(df[text_col].apply(lambda x: \" \".join(x)))\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "    \n",
    "    group_keywords = {}\n",
    "    for group in df[group_col].unique():\n",
    "        group_df = df[df[group_col] == group]\n",
    "        group_tfidf_scores = tfidf_scores.iloc[group_df.index, :]\n",
    "        \n",
    "        group_mean = group_tfidf_scores.mean()\n",
    "        group_mean_sorted = group_mean.sort_values(ascending=False)\n",
    "        \n",
    "        all_other_groups_df = df[df[group_col] != group]\n",
    "        all_other_groups_tfidf_scores = tfidf_scores.iloc[all_other_groups_df.index, :]\n",
    "        all_other_groups_mean = all_other_groups_tfidf_scores.mean()\n",
    "        \n",
    "        more_present_keywords = []\n",
    "        for i, (word, score) in enumerate(group_mean_sorted.iteritems()):\n",
    "            if score > all_other_groups_mean[word]:\n",
    "                more_present_keywords.append(word)\n",
    "            if i >= n:\n",
    "                break\n",
    "        \n",
    "        group_keywords[group] = more_present_keywords\n",
    "        \n",
    "    return group_keywords\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_top_group_keywords(df, group_col, text_col, n=10, group_values=None, tfidf_params=None, sort=False, compare_with_all=False):\n",
    "    \"\"\"\n",
    "    Extract the top `n` keywords that are more present in a subgroup compared to the rest of the corpus (all other groups).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    group_col : str\n",
    "        Column in `df` that contains the subgroup information.\n",
    "    text_col : str\n",
    "        Column in `df` that contains the text data.\n",
    "    n : int, optional (default=10)\n",
    "        Number of keywords to extract for each subgroup.\n",
    "    group_values : list, optional (default=None)\n",
    "        List of values in `group_col` to include in the analysis. If None, include all unique values.\n",
    "    tfidf_params : dict, optional (default=None)\n",
    "        Dictionary of parameters to pass to `TfidfVectorizer`.\n",
    "    sort : bool, optional (default=False)\n",
    "        Sort the keywords by their scores in each group.\n",
    "    compare_with_overall : bool, optional (default=False)\n",
    "        Compare the keyword scores of each group with the overall mean of the corpus instead of with the scores of all other groups.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    group_keywords : dict\n",
    "        Dictionary where the keys are the subgroup names and the values are lists of the keyword scores in each subgroup.\n",
    "    \"\"\"\n",
    "\n",
    "    if group_values:\n",
    "        df = df[df[group_col].isin(group_values)]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**(tfidf_params if tfidf_params else {}))\n",
    "    X = vectorizer.fit_transform(df[text_col].apply(lambda x: \" \".join(x)))\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "    group_keywords = {}\n",
    "    all_group_mean = tfidf_scores.mean()\n",
    "    for group in df[group_col].unique():\n",
    "        group_df = df[df[group_col] == group]\n",
    "        group_tfidf_scores = tfidf_scores.iloc[group_df.index, :]\n",
    "        \n",
    "        group_mean = group_tfidf_scores.mean()\n",
    "        if sort:\n",
    "            group_mean_sorted = group_mean.sort_values(ascending=False)\n",
    "        else:\n",
    "            group_mean_sorted = group_mean\n",
    "        \n",
    "        if compare_with_all:\n",
    "            all_other_groups_mean = all_group_mean\n",
    "        else:\n",
    "            all_other_groups_df = df[df[group_col] != group]\n",
    "            all_other_groups_tfidf_scores = tfidf_scores.iloc[all_other_groups_df.index, :]\n",
    "            all_other_groups_mean = all_other_groups_tfidf_scores.mean()\n",
    "        \n",
    "        more_present_keywords = []\n",
    "        for i, (word, score) in enumerate(group_mean_sorted.iteritems()):\n",
    "            if score > all_other_groups_mean[word]:\n",
    "                more_present_keywords.append((word, score))\n",
    "            if i >= n:\n",
    "                break\n",
    "        \n",
    "        group_keywords[group] = more_present_keywords\n",
    "    \n",
    "        \n",
    "    return group_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_group_representative_keywords(df, group_col, text_col, n=10, group_values=None, tfidf_params=None, sort=False):\n",
    "    if group_values:\n",
    "        df = df[df[group_col].isin(group_values)]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**(tfidf_params if tfidf_params else {}))\n",
    "    X = vectorizer.fit_transform(df[text_col].apply(lambda x: \" \".join(x)))\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "    group_representative_keywords = {}\n",
    "    for group in df[group_col].unique():\n",
    "        group_df = df[df[group_col] == group]\n",
    "        group_tfidf_scores = tfidf_scores.iloc[group_df.index, :]\n",
    "        group_mean = group_tfidf_scores.mean()\n",
    "\n",
    "        if sort:\n",
    "            group_mean_sorted = group_mean.sort_values(ascending=False)\n",
    "        else:\n",
    "            group_mean_sorted = group_mean\n",
    "\n",
    "        representative_keywords = []\n",
    "        for i, (word, score) in enumerate(group_mean_sorted.iteritems()):\n",
    "            representative_keywords.append((word, score))\n",
    "            if i >= n:\n",
    "                break\n",
    "        \n",
    "        group_representative_keywords[group] = representative_keywords\n",
    "    \n",
    "    return group_representative_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def category_based_tfidf(df, column, category_column, tfidf_params=None):\n",
    "    # create a dict with the frequency of each word in each category\n",
    "    freq = defaultdict(lambda: defaultdict(int))\n",
    "    for i, row in df.iterrows():\n",
    "        category = row[category_column]\n",
    "        if isinstance(row[column], list):\n",
    "            for sublist in row[column]:\n",
    "                if isinstance(sublist, np.ndarray):\n",
    "                    sublist = sublist.tolist()\n",
    "                for word in str(sublist).split():\n",
    "                    freq[category][word] += 1\n",
    "        else:\n",
    "            for word in str(row[column]).split():\n",
    "                freq[category][word] += 1\n",
    "    # convert the dict to a dataframe for further processing\n",
    "    freq_df = pd.DataFrame(freq).transpose().fillna(0)\n",
    "    # convert the frequency data into a list of strings\n",
    "    strings = []\n",
    "    for i, row in freq_df.iterrows():\n",
    "        strings.append(\" \".join([f\"{k}:{v}\" for k,v in row.to_dict().items()]))\n",
    "    # calculate tf-idf for each word in each category\n",
    "    if tfidf_params is None:\n",
    "        vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(use_idf=True, **tfidf_params)\n",
    "    tfidf = vectorizer.fit_transform(strings)\n",
    "    # return the tf-idf score in a dataframe\n",
    "    return pd.DataFrame(tfidf.todense(), index=freq_df.index, columns=vectorizer.get_feature_names_out()).transpose()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df\n",
    "tfidf_params={\"max_df\": 0.9, \"min_df\": 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = category_based_tfidf(df_test, 'nouns', 'season',tfidf_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>u_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>yearmonth</th>\n",
       "      <th>yearquarter</th>\n",
       "      <th>season</th>\n",
       "      <th>Kommentar</th>\n",
       "      <th>wime_personal</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_zielort</th>\n",
       "      <th>Kommentar_Character</th>\n",
       "      <th>Kommentar_Tokens</th>\n",
       "      <th>Kommentar_Types</th>\n",
       "      <th>Kommentar_TTR</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_no_loc</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>612374</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022/4</td>\n",
       "      <td>winter</td>\n",
       "      <td>Häufigere Verbindungen zw. Bern-Luzern.</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>häufigere verbindungen zw. bern-luzern.</td>\n",
       "      <td>[bern, luzern, .]</td>\n",
       "      <td>[--]</td>\n",
       "      <td>[--]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>604479</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022/4</td>\n",
       "      <td>winter</td>\n",
       "      <td>Als Student ist das GA Preiswert, aber mehr wü...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>BÜREN AN DER AARE</td>\n",
       "      <td>180</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>student ga preiswert, gerne bezahlen. 1 monats...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>604528</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022/4</td>\n",
       "      <td>winter</td>\n",
       "      <td>Weniger 1. Klasswagons und mehr 2. Klassenwago...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Bern</td>\n",
       "      <td>140</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>95.652174</td>\n",
       "      <td>1. klasswagons 2. klassenwagons. genau ir16 (b...</td>\n",
       "      <td>[(, bern, aarau, aarau, bern, )]</td>\n",
       "      <td>[--, --]</td>\n",
       "      <td>[--, --]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604518</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022/4</td>\n",
       "      <td>winter</td>\n",
       "      <td>Immer wieder grosses Problem ist kaputt gescho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ZÜRICH HB</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>problem kaputt geschossen wc. züge (s9).</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604504</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022/4</td>\n",
       "      <td>winter</td>\n",
       "      <td>Ich bin Pendlerin ( und glückliche Inhaberin e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Chur</td>\n",
       "      <td>331</td>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>88.461538</td>\n",
       "      <td>pendlerin ( glückliche inhaberin gas) pendle f...</td>\n",
       "      <td>[zweitklass, wagons]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64340</th>\n",
       "      <td>51540</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Dieser Zug war ab Zürich HB ein Nachtzug. Die ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Stettbach</td>\n",
       "      <td>201</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>zug zürich hb nachtzug. app nachtzuschlag verr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64341</th>\n",
       "      <td>51531</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Übersichtlichkeit der App.Es könnte besser ang...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Basel SBB</td>\n",
       "      <td>185</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>übersichtlichkeit app.es angegeben werden, tic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64342</th>\n",
       "      <td>51529</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Bei der Rückfahrt hatten wir Sitzplätze aber b...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Thun</td>\n",
       "      <td>450</td>\n",
       "      <td>73</td>\n",
       "      <td>60</td>\n",
       "      <td>82.191781</td>\n",
       "      <td>rückfahrt sitzplätze anreise thun interlaken s...</td>\n",
       "      <td>[sind:, )]</td>\n",
       "      <td>[--]</td>\n",
       "      <td>[--]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64343</th>\n",
       "      <td>51527</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Mehr Spielwagen auf unterschiedlichen Strecken...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Chur</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>spielwagen unterschiedlichen strecken wären su...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64344</th>\n",
       "      <td>51405</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2019/1</td>\n",
       "      <td>winter</td>\n",
       "      <td>Die Tickets günstiger machen, damit man auch l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Zürich HB</td>\n",
       "      <td>78</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>tickets günstiger machen, zug unterwegs ist.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64345 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       participant_id     u_date  year  month  quarter  yearmonth yearquarter  \\\n",
       "0              612374 2022-12-31  2022     12        4 2022-12-01      2022/4   \n",
       "1              604479 2022-12-31  2022     12        4 2022-12-01      2022/4   \n",
       "2              604528 2022-12-31  2022     12        4 2022-12-01      2022/4   \n",
       "3              604518 2022-12-31  2022     12        4 2022-12-01      2022/4   \n",
       "4              604504 2022-12-31  2022     12        4 2022-12-01      2022/4   \n",
       "...               ...        ...   ...    ...      ...        ...         ...   \n",
       "64340           51540 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "64341           51531 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "64342           51529 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "64343           51527 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "64344           51405 2019-01-01  2019      1        1 2019-01-01      2019/1   \n",
       "\n",
       "       season                                          Kommentar  \\\n",
       "0      winter            Häufigere Verbindungen zw. Bern-Luzern.   \n",
       "1      winter  Als Student ist das GA Preiswert, aber mehr wü...   \n",
       "2      winter  Weniger 1. Klasswagons und mehr 2. Klassenwago...   \n",
       "3      winter  Immer wieder grosses Problem ist kaputt gescho...   \n",
       "4      winter  Ich bin Pendlerin ( und glückliche Inhaberin e...   \n",
       "...       ...                                                ...   \n",
       "64340  winter  Dieser Zug war ab Zürich HB ein Nachtzug. Die ...   \n",
       "64341  winter  Übersichtlichkeit der App.Es könnte besser ang...   \n",
       "64342  winter  Bei der Rückfahrt hatten wir Sitzplätze aber b...   \n",
       "64343  winter  Mehr Spielwagen auf unterschiedlichen Strecken...   \n",
       "64344  winter  Die Tickets günstiger machen, damit man auch l...   \n",
       "\n",
       "       wime_personal  ...         ft_zielort  Kommentar_Character  \\\n",
       "0               75.0  ...             Luzern                   39   \n",
       "1                NaN  ...  BÜREN AN DER AARE                  180   \n",
       "2              100.0  ...               Bern                  140   \n",
       "3                NaN  ...          ZÜRICH HB                   75   \n",
       "4                NaN  ...               Chur                  331   \n",
       "...              ...  ...                ...                  ...   \n",
       "64340            NaN  ...          Stettbach                  201   \n",
       "64341          100.0  ...          Basel SBB                  185   \n",
       "64342          100.0  ...               Thun                  450   \n",
       "64343          100.0  ...               Chur                   59   \n",
       "64344            0.0  ...          Zürich HB                   78   \n",
       "\n",
       "       Kommentar_Tokens  Kommentar_Types  Kommentar_TTR  \\\n",
       "0                     4                4     100.000000   \n",
       "1                    32               28      87.500000   \n",
       "2                    23               22      95.652174   \n",
       "3                    12               12     100.000000   \n",
       "4                    52               46      88.461538   \n",
       "...                 ...              ...            ...   \n",
       "64340                34               32      94.117647   \n",
       "64341                23               23     100.000000   \n",
       "64342                73               60      82.191781   \n",
       "64343                 7                7     100.000000   \n",
       "64344                13               13     100.000000   \n",
       "\n",
       "                                       text_preprocessed  \\\n",
       "0                häufigere verbindungen zw. bern-luzern.   \n",
       "1      student ga preiswert, gerne bezahlen. 1 monats...   \n",
       "2      1. klasswagons 2. klassenwagons. genau ir16 (b...   \n",
       "3               problem kaputt geschossen wc. züge (s9).   \n",
       "4      pendlerin ( glückliche inhaberin gas) pendle f...   \n",
       "...                                                  ...   \n",
       "64340  zug zürich hb nachtzug. app nachtzuschlag verr...   \n",
       "64341  übersichtlichkeit app.es angegeben werden, tic...   \n",
       "64342  rückfahrt sitzplätze anreise thun interlaken s...   \n",
       "64343  spielwagen unterschiedlichen strecken wären su...   \n",
       "64344       tickets günstiger machen, zug unterwegs ist.   \n",
       "\n",
       "                              tokenized  lemmatized lemmatized_no_loc  nouns  \n",
       "0                     [bern, luzern, .]        [--]              [--]     []  \n",
       "1                                    []          []                []     []  \n",
       "2      [(, bern, aarau, aarau, bern, )]    [--, --]          [--, --]     []  \n",
       "3                                    []          []                []     []  \n",
       "4                  [zweitklass, wagons]          []                []     []  \n",
       "...                                 ...         ...               ...    ...  \n",
       "64340                                []          []                []     []  \n",
       "64341                                []          []                []     []  \n",
       "64342                        [sind:, )]        [--]              [--]     []  \n",
       "64343                                []          []                []     []  \n",
       "64344                                []          []                []     []  \n",
       "\n",
       "[64345 rows x 56 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winter</th>\n",
       "      <th>autumn</th>\n",
       "      <th>summer</th>\n",
       "      <th>spring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.02717</td>\n",
       "      <td>0.028286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00fr</th>\n",
       "      <td>0.005817</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.00572</td>\n",
       "      <td>0.005955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00uhr</th>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.017194</td>\n",
       "      <td>0.01716</td>\n",
       "      <td>0.017865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.01430</td>\n",
       "      <td>0.014887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>übrigen</th>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>übrtprüft</th>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>übung</th>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.00429</td>\n",
       "      <td>0.004466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>übungen</th>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ürkheim</th>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             winter    autumn   summer    spring\n",
       "00         0.027633  0.027223  0.02717  0.028286\n",
       "000        0.001454  0.001433  0.00143  0.001489\n",
       "00fr       0.005817  0.005731  0.00572  0.005955\n",
       "00uhr      0.017452  0.017194  0.01716  0.017865\n",
       "01         0.014544  0.014328  0.01430  0.014887\n",
       "...             ...       ...      ...       ...\n",
       "übrigen    0.001454  0.001433  0.00143  0.001489\n",
       "übrtprüft  0.001454  0.001433  0.00143  0.001489\n",
       "übung      0.004363  0.004298  0.00429  0.004466\n",
       "übungen    0.001454  0.001433  0.00143  0.001489\n",
       "ürkheim    0.001454  0.001433  0.00143  0.001489\n",
       "\n",
       "[24300 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95     0.242775\n",
       "92     0.242775\n",
       "75     0.242775\n",
       "71     0.242775\n",
       "159    0.190177\n",
       "         ...   \n",
       "273    0.000000\n",
       "269    0.000000\n",
       "268    0.000000\n",
       "263    0.000000\n",
       "101    0.000000\n",
       "Name: summer, Length: 178, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['summer'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 3 keywords only for the Female group\n",
    "group_values = list(df.S_sex.unique())\n",
    "tfidf_params = {\n",
    "    #'min_df': 0.001,\n",
    "    #'max_df': 0.5,\n",
    "    'ngram_range':(1,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/zp8d2_c16z32wsh2hdjb9fch0000gn/T/ipykernel_34856/1578814117.py:63: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for i, (word, score) in enumerate(group_mean_sorted.iteritems()):\n"
     ]
    }
   ],
   "source": [
    "top_keywords_new = extract_top_group_keywords(df_test, 'S_sex', 'nouns', n=10, group_values=group_values, tfidf_params=tfidf_params, sort=True, compare_with_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_keywords \u001b[39m=\u001b[39m extract_group_representative_keywords(df_test, \u001b[39m'\u001b[39;49m\u001b[39myearquarter\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnouns\u001b[39;49m\u001b[39m'\u001b[39;49m, n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, group_values\u001b[39m=\u001b[39;49mgroup_values, tfidf_params\u001b[39m=\u001b[39;49mtfidf_params, sort\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mextract_group_representative_keywords\u001b[0;34m(df, group_col, text_col, n, group_values, tfidf_params, sort)\u001b[0m\n\u001b[1;32m      3\u001b[0m     df \u001b[39m=\u001b[39m df[df[group_col]\u001b[39m.\u001b[39misin(group_values)]\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(tfidf_params \u001b[39mif\u001b[39;00m tfidf_params \u001b[39melse\u001b[39;00m {}))\n\u001b[0;32m----> 6\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(df[text_col]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(x)))\n\u001b[1;32m      7\u001b[0m feature_names \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[1;32m      8\u001b[0m tfidf_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39mfeature_names)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2116\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2117\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2118\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2119\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2120\u001b[0m )\n\u001b[0;32m-> 2121\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2123\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1374\u001b[0m             )\n\u001b[1;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1283\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1284\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1285\u001b[0m         )\n\u001b[1;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "top_keywords = extract_group_representative_keywords(df_test, 'yearquarter', 'nouns', n=10, group_values=group_values, tfidf_params=tfidf_params, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(group_keywords, n=10):\n",
    "    groups = list(group_keywords.keys())\n",
    "    keywords = []\n",
    "    for i in range(n):\n",
    "        keyword_column = []\n",
    "        for group in groups:\n",
    "            try:\n",
    "                keyword_column.append(group_keywords[group][i])\n",
    "            except IndexError:\n",
    "                keyword_column.append(None)\n",
    "        keywords.append(keyword_column)\n",
    "\n",
    "    df = pd.DataFrame({'Group': groups})\n",
    "    for i, keyword_column in enumerate(keywords):\n",
    "        df[f'Keyword_{i+1}'] = keyword_column\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Keyword_1</th>\n",
       "      <th>Keyword_2</th>\n",
       "      <th>Keyword_3</th>\n",
       "      <th>Keyword_4</th>\n",
       "      <th>Keyword_5</th>\n",
       "      <th>Keyword_6</th>\n",
       "      <th>Keyword_7</th>\n",
       "      <th>Keyword_8</th>\n",
       "      <th>Keyword_9</th>\n",
       "      <th>Keyword_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weiblich</td>\n",
       "      <td>(zug, 0.07613526190015996)</td>\n",
       "      <td>(züge, 0.03198508972572204)</td>\n",
       "      <td>(bahnhof, 0.020381162425043457)</td>\n",
       "      <td>(verspätung, 0.015890812059224952)</td>\n",
       "      <td>(app, 0.015766849563078514)</td>\n",
       "      <td>(fahrt, 0.015387310939313703)</td>\n",
       "      <td>(strecke, 0.013799337069380286)</td>\n",
       "      <td>(verbindung, 0.013339009915221332)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>männlich</td>\n",
       "      <td>(klasse, 0.03038435640214663)</td>\n",
       "      <td>(platz, 0.021654728275532373)</td>\n",
       "      <td>(minuten, 0.010923924368636626)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>divers</td>\n",
       "      <td>(fragen, 0.0517727650996974)</td>\n",
       "      <td>(bahnhof, 0.050282481323229905)</td>\n",
       "      <td>(verbindungen, 0.0473328137881326)</td>\n",
       "      <td>(pünktlichkeit, 0.0469683049856828)</td>\n",
       "      <td>(platz, 0.041575886571870314)</td>\n",
       "      <td>(krise, 0.03125)</td>\n",
       "      <td>(wlan, 0.03125)</td>\n",
       "      <td>(klasse, 0.03125)</td>\n",
       "      <td>(kinderwagen, 0.03125)</td>\n",
       "      <td>(takt, 0.03125)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Group                      Keyword_1                        Keyword_2  \\\n",
       "0  weiblich     (zug, 0.07613526190015996)      (züge, 0.03198508972572204)   \n",
       "1  männlich  (klasse, 0.03038435640214663)    (platz, 0.021654728275532373)   \n",
       "2    divers   (fragen, 0.0517727650996974)  (bahnhof, 0.050282481323229905)   \n",
       "\n",
       "                            Keyword_3                            Keyword_4  \\\n",
       "0     (bahnhof, 0.020381162425043457)   (verspätung, 0.015890812059224952)   \n",
       "1     (minuten, 0.010923924368636626)                                 None   \n",
       "2  (verbindungen, 0.0473328137881326)  (pünktlichkeit, 0.0469683049856828)   \n",
       "\n",
       "                       Keyword_5                      Keyword_6  \\\n",
       "0    (app, 0.015766849563078514)  (fahrt, 0.015387310939313703)   \n",
       "1                           None                           None   \n",
       "2  (platz, 0.041575886571870314)               (krise, 0.03125)   \n",
       "\n",
       "                         Keyword_7                           Keyword_8  \\\n",
       "0  (strecke, 0.013799337069380286)  (verbindung, 0.013339009915221332)   \n",
       "1                             None                                None   \n",
       "2                  (wlan, 0.03125)                   (klasse, 0.03125)   \n",
       "\n",
       "                Keyword_9       Keyword_10  \n",
       "0                    None             None  \n",
       "1                    None             None  \n",
       "2  (kinderwagen, 0.03125)  (takt, 0.03125)  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df(top_keywords_new,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81093347139ca5f3fb4cd58be29463cca1247dcd7b103014feebee6100f3ad2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
