{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertTopic\n",
    "In diesem Notebook trainieren wir die BERTTopic Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-06 06:47:31.676816: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_path = \"/models\" # Models werden extern abgelegt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>u_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>yearmonth</th>\n",
       "      <th>yearquarter</th>\n",
       "      <th>season</th>\n",
       "      <th>Kommentar</th>\n",
       "      <th>wime_personal</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_zielort</th>\n",
       "      <th>Kommentar_Character</th>\n",
       "      <th>Kommentar_Tokens</th>\n",
       "      <th>Kommentar_Types</th>\n",
       "      <th>Kommentar_TTR</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_no_loc</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>612374</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022/4</td>\n",
       "      <td>winter</td>\n",
       "      <td>Häufigere Verbindungen zw. Bern-Luzern.</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>häufigere verbindungen zw. bern-luzern.</td>\n",
       "      <td>[häufigere, verbindungen, zw, ., bern-luzern, .]</td>\n",
       "      <td>[häufig, verbindung, zw, --, bern-luzern, --]</td>\n",
       "      <td>[häufig, verbindung, zw, --, bern-luzern, --]</td>\n",
       "      <td>[verbindungen, bern-luzern]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id     u_date  year  month  quarter  yearmonth yearquarter  \\\n",
       "0          612374 2022-12-31  2022     12        4 2022-12-01      2022/4   \n",
       "\n",
       "   season                                Kommentar  wime_personal  ...  \\\n",
       "0  winter  Häufigere Verbindungen zw. Bern-Luzern.           75.0  ...   \n",
       "\n",
       "   ft_zielort  Kommentar_Character  Kommentar_Tokens  Kommentar_Types  \\\n",
       "0      Luzern                   39                 4                4   \n",
       "\n",
       "   Kommentar_TTR                        text_preprocessed  \\\n",
       "0          100.0  häufigere verbindungen zw. bern-luzern.   \n",
       "\n",
       "                                          tokenized  \\\n",
       "0  [häufigere, verbindungen, zw, ., bern-luzern, .]   \n",
       "\n",
       "                                      lemmatized  \\\n",
       "0  [häufig, verbindung, zw, --, bern-luzern, --]   \n",
       "\n",
       "                               lemmatized_no_loc                        nouns  \n",
       "0  [häufig, verbindung, zw, --, bern-luzern, --]  [verbindungen, bern-luzern]  \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import dataframe\n",
    "filelocation = 'data/DataText'\n",
    "data = pd.read_feather(filelocation)\n",
    "data.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"text_preprocessed\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def fit_berttopic(target_dir:str, embedding_model=None, min_topic_size:int=50) -> None:\n",
    "    \"\"\"\n",
    "        Trainiert und speichert BERTTopic Modell \n",
    "        https://maartengr.github.io/BERTopic/index.html\n",
    "\n",
    "        :param target_dir: Speicherort für fertiges Modell\n",
    "        :param embedding_model: Name des Embedding-Modells oder ein Modell\n",
    "        :param min_topic_size: minimale Größe eines Topics (HDBSCAN Clusters)\n",
    "\n",
    "    \"\"\"\n",
    "    german_stop_words = stopwords.words('german') # Stopwords für Keyword-Berechnung\n",
    "\n",
    "    # CountVectorizer ist Standard aber extern definiert, um deutsche Stopwords zu nutzen\n",
    "    vectorizer = CountVectorizer(stop_words=german_stop_words) \n",
    "    model = BERTopic(\n",
    "        language=\"german\",\n",
    "        vectorizer_model=vectorizer,\n",
    "        embedding_model=embedding_model,\n",
    "        min_topic_size=min_topic_size)\n",
    "\n",
    "    topics, probs = model.fit_transform(docs)\n",
    "\n",
    "    model.save(target_dir)\n",
    "\n",
    "def fit_berttopic_if_not_exists(target_dir, embedding_model=None, min_topic_size:int=50) -> None:\n",
    "    \"\"\"\n",
    "        Wrapper für fit_berttopic damit bereits trainierte Modelle nicht erneut trainiert werden\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(target_dir):\n",
    "        print(\"Model already trained\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Fitting Model {embedding_model}...\")\n",
    "        fit_berttopic(target_dir=target_dir, embedding_model=embedding_model, min_topic_size=min_topic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 471M/471M [06:53<00:00, 1.14MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 22.8kB/s]\n",
      "Downloading: 100%|██████████| 5.07M/5.07M [00:04<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 105kB/s]\n",
      "Downloading: 100%|██████████| 9.08M/9.08M [00:07<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 480/480 [00:00<00:00, 255kB/s]\n",
      "Downloading: 100%|██████████| 14.8M/14.8M [00:12<00:00, 1.16MB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 99.9kB/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Topic Modell Erstellung auf Basis des BERT-Embeddings\n",
    "fit_berttopic_if_not_exists(model_base_path + \"/BERTTopic.model\", min_topic_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modell Erstellung auf Basis des Spacy-Embeddings\n",
    "nlp = spacy.load('de_core_news_sm', exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "\n",
    "# Modell Training\n",
    "fit_berttopic_if_not_exists(model_base_path + \"/BERTTopic_spacy.model\",embedding_model=nlp, min_topic_size=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregierte Topics\n",
    "Hier aggregieren die Anzahl der Topics auf 10, um einen Überblick zu bekommen.\n",
    "- Dabei wird iterativ das kleinste Topic mit dem ähnlichsten zusammengefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_base_path + \"/BERTTopic_aggregated_10.model\"):\n",
    "    model = BERTopic.load(model_base_path + \"/BERTTopic.model\")\n",
    "    model.reduce_topics(docs=docs, nr_topics=10)\n",
    "    model.save(model_base_path + \"/BERTTopic_aggregated_10.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81093347139ca5f3fb4cd58be29463cca1247dcd7b103014feebee6100f3ad2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
