{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertTopic\n",
    "In diesem Notebook trainieren wir die BERTTopic Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 15:30:15.270236: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../')) ## needed to import the function.py file\n",
    "\n",
    "from functions import *\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from spacy.lang.de.stop_words import STOP_WORDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe\n",
    "filelocation = '../../data/DataClean'\n",
    "df = pd.read_feather(filelocation)\n",
    "\n",
    "# import custom stopwords list\n",
    "customstopwords = pd.read_excel('../../config/customstopwords.xlsx')\n",
    "customstopwords = customstopwords['stopword'].tolist()\n",
    "\n",
    "customstopwords.extend(STOP_WORDS) # also add spacy stopwords\n",
    "\n",
    "# Also add ortsnamen to the stoplist because we have them in the metadata and dont want them in the comments\n",
    "orte = [x.lower() for x in set(df.ft_startort.tolist()) if x == x and x.lower() != '']\n",
    "\n",
    "# Create the list of locations\n",
    "for location in df.ft_startort.tolist():\n",
    "    # Check if the value is a string\n",
    "    if isinstance(location, str):\n",
    "        # Convert to lowercase and remove 'Zug'\n",
    "        location = location.lower()\n",
    "        if location == 'zug':\n",
    "            continue\n",
    "        \n",
    "        # Split the location into tokens if it contains whitespace\n",
    "        tokens = location.split()\n",
    "        \n",
    "        # Add each token to the list individually\n",
    "        for token in tokens:\n",
    "            # Skip any token that is in the stoplist\n",
    "            if token in orte:\n",
    "                continue\n",
    "            # Remove any commas from the end of the token\n",
    "            token = token.rstrip(',')\n",
    "            orte.append(token)\n",
    "    \n",
    "# Remove duplicates from the list\n",
    "orte = list(set(orte))\n",
    "\n",
    "orte.remove(\"zug\")\n",
    "\n",
    "# extend the stopword list with the ortsnamen\n",
    "customstopwords.extend(orte)\n",
    "customstopwords = list(dict.fromkeys(customstopwords)) #remove potenzial duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe\n",
    "filelocation = '../../data/DataTextTrain'\n",
    "data = pd.read_feather(filelocation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"Kommentar\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters for all models\n",
    "min_topic_size=50\n",
    "stop_words=customstopwords\n",
    "modelpath = \"../../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1: High Speed / Small Size / Lower Benchmark Scores\n",
    "modelname = 'paraphrase-MiniLM-L3-v2'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while fitting BERTopic model: __init__() got an unexpected keyword argument 'stop_words'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train Model 2: High Speed / Medium Size / Medium Benchmark Scores\u001b[39;00m\n\u001b[1;32m      2\u001b[0m modelname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m fit_berttopic_if_not_exists(modelpath\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBERTTopic_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m modelname \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.model\u001b[39;49m\u001b[39m\"\u001b[39;49m,docs\u001b[39m=\u001b[39;49mdocs,embedding_model\u001b[39m=\u001b[39;49mmodelname, min_topic_size\u001b[39m=\u001b[39;49mmin_topic_size,stop_words\u001b[39m=\u001b[39;49mstop_words)\n",
      "File \u001b[0;32m~/Repos/github.com/DominikFin/nlp-satisfaction/code/functions.py:802\u001b[0m, in \u001b[0;36mfit_berttopic_if_not_exists\u001b[0;34m(target_dir, docs, embedding_model, min_topic_size, stop_words)\u001b[0m\n\u001b[1;32m    799\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel already trained at \u001b[39m\u001b[39m{\u001b[39;00mtarget_dir\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    800\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 802\u001b[0m fit_berttopic(target_dir, docs, embedding_model, min_topic_size, stop_words)\n",
      "File \u001b[0;32m~/Repos/github.com/DominikFin/nlp-satisfaction/code/functions.py:764\u001b[0m, in \u001b[0;36mfit_berttopic\u001b[0;34m(target_dir, docs, embedding_model, min_topic_size, stop_words)\u001b[0m\n\u001b[1;32m    762\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFitting BERTopic model with \u001b[39m\u001b[39m{\u001b[39;00membedding_model\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    763\u001b[0m german_stop_words \u001b[39m=\u001b[39m stop_words \u001b[39mor\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m\"\u001b[39m\u001b[39mgerman\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 764\u001b[0m model \u001b[39m=\u001b[39m BERTopic(\n\u001b[1;32m    765\u001b[0m     language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgerman\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    766\u001b[0m     stop_words\u001b[39m=\u001b[39;49mgerman_stop_words,\n\u001b[1;32m    767\u001b[0m     embedding_model\u001b[39m=\u001b[39;49membedding_model,\n\u001b[1;32m    768\u001b[0m     min_topic_size\u001b[39m=\u001b[39;49mmin_topic_size)\n\u001b[1;32m    769\u001b[0m topics, probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit_transform(docs)\n\u001b[1;32m    771\u001b[0m new_topics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mreduce_outliers(docs, topics) \u001b[39m# Reduce outliers\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'stop_words'"
     ]
    }
   ],
   "source": [
    "# Train Model 2: High Speed / Medium Size / Medium Benchmark Scores\n",
    "modelname = 'all-MiniLM-L6-v2'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Train Model 3: Low Speed / High Size / Medium-High Benchmark Scores\n",
    "modelname = 'all-MiniLM-L12-v2'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Train Model 4: Low Speed / High Size / High Benchmark Scores (02.2023)\n",
    "modelname = 'all-distilroberta-v1'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Train Model 5: Low Speed / High Size / High Benchmark Scores (02.2023)\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_default.model\",docs=docs, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c54f9282dd36543c8181ef7676bc28d81677c039b129753722332dc3d171a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
