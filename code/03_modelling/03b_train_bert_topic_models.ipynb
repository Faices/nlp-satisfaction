{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertTopic\n",
    "In diesem Notebook trainieren wir die BERTTopic Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../')) ## needed to import the function.py file\n",
    "\n",
    "from functions import *\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from spacy.lang.de.stop_words import STOP_WORDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe\n",
    "filelocation = '../../data/DataClean'\n",
    "df = pd.read_feather(filelocation)\n",
    "\n",
    "# import custom stopwords list\n",
    "customstopwords = pd.read_excel('../../config/customstopwords.xlsx')\n",
    "customstopwords = customstopwords['stopword'].tolist()\n",
    "\n",
    "customstopwords.extend(STOP_WORDS) # also add spacy stopwords\n",
    "\n",
    "# Also add ortsnamen to the stoplist because we have them in the metadata and dont want them in the comments\n",
    "orte = [x.lower() for x in set(df.ft_startort.tolist()) if x == x and x.lower() != '']\n",
    "\n",
    "# Create the list of locations\n",
    "for location in df.ft_startort.tolist():\n",
    "    # Check if the value is a string\n",
    "    if isinstance(location, str):\n",
    "        # Convert to lowercase and remove 'Zug'\n",
    "        location = location.lower()\n",
    "        if location == 'zug':\n",
    "            continue\n",
    "        \n",
    "        # Split the location into tokens if it contains whitespace\n",
    "        tokens = location.split()\n",
    "        \n",
    "        # Add each token to the list individually\n",
    "        for token in tokens:\n",
    "            # Skip any token that is in the stoplist\n",
    "            if token in orte:\n",
    "                continue\n",
    "            # Remove any commas from the end of the token\n",
    "            token = token.rstrip(',')\n",
    "            orte.append(token)\n",
    "    \n",
    "# Remove duplicates from the list\n",
    "orte = list(set(orte))\n",
    "\n",
    "orte.remove(\"zug\")\n",
    "\n",
    "# extend the stopword list with the ortsnamen\n",
    "customstopwords.extend(orte)\n",
    "customstopwords = list(dict.fromkeys(customstopwords)) #remove potenzial duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe\n",
    "filelocation = '../../data/DataTextTrain'\n",
    "data = pd.read_feather(filelocation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"Kommentar\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters for all models\n",
    "min_topic_size=50\n",
    "stop_words=customstopwords\n",
    "modelpath = \"../../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1: High Speed / Small Size / Lower Benchmark Scores\n",
    "modelname = 'paraphrase-MiniLM-L3-v2'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Train Model 2: High Speed / Medium Size / Medium Benchmark Scores\n",
    "modelname = 'all-MiniLM-L6-v2'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3: Low Speed / High Size / Medium-High Benchmark Scores\n",
    "modelname = 'all-MiniLM-L12-v2'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 4: Low Speed / High Size / High Benchmark Scores (02.2023)\n",
    "modelname = 'all-distilroberta-v1'\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_\" + modelname + \".model\",docs=docs,embedding_model=modelname, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dom/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Train Model 5: Low Speed / High Size / High Benchmark Scores (02.2023)\n",
    "fit_berttopic_if_not_exists(modelpath+\"BERTTopic_default.model\",docs=docs, min_topic_size=min_topic_size,stop_words=stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c54f9282dd36543c8181ef7676bc28d81677c039b129753722332dc3d171a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
